{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now that we have our cleaned dataset, we will apply Natural Language Processing Techniques like\n",
    "- BoW(Bag of Words)\n",
    "- Tf-IDf\n",
    "- Word2Vec \n",
    "\n",
    "##### in order to convert the text to vectors so that we will be able to apply linear algebra techniques later for predicting the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>bought sever vital can dog food product found ...</td>\n",
       "      <td>postive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>product arriv label jumbo salt peanutsth peanu...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>confect around centuri light pillowi citru gel...</td>\n",
       "      <td>postive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>look secret ingredi robitussin believ found go...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>great taffi great price wide assort yummi taff...</td>\n",
       "      <td>postive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Id   ProductId          UserId                      ProfileName  \\\n",
       "0      0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1      1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2      2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3      3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4      4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \\\n",
       "0  Good Quality Dog Food  bought sever vital can dog food product found ...   \n",
       "1      Not as Advertised  product arriv label jumbo salt peanutsth peanu...   \n",
       "2  \"Delight\" says it all  confect around centuri light pillowi citru gel...   \n",
       "3         Cough Medicine  look secret ingredi robitussin believ found go...   \n",
       "4            Great taffy  great taffi great price wide assort yummi taff...   \n",
       "\n",
       "  sentiment  \n",
       "0   postive  \n",
       "1  negative  \n",
       "2   postive  \n",
       "3  negative  \n",
       "4   postive  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets reload the data from the SQLite file\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "con = sqlite3.connect('final_cleaned.sqlite')\n",
    "cleaned_data = pd.read_sql_query('select * \\\n",
    "                                 from Reviews',con)\n",
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = cleaned_data[cleaned_data.sentiment == 'postive'].sample(2000)\n",
    "negative_data = cleaned_data[cleaned_data.sentiment == 'negative'].sample(2000)\n",
    "\n",
    "data_4k = pd.concat([positive_data,negative_data])\n",
    "sentiment_4k = data_4k['sentiment']\n",
    "data_4k = data_4k.drop(['sentiment'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_4k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_4k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# count_vectorizer = CountVectorizer().fit(data_4k.Text.values)\n",
    "# final_counts = count_vectorizer.transform(data_4k)\n",
    "# final_counts.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are many new columns formed and we know that\n",
    "# BoW does not perform well\n",
    "# as it does not consider the sequence information\n",
    "\n",
    "# The advantage of using n-grams is, it considers a pair of \n",
    "# consequent words.\n",
    "# if we use simple bag of words by using stop words\n",
    "# it will lead to miss classification \n",
    "# in some cases,so it is always good to use n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stop words like \"not\" should be avoided before building n-grams\n",
    "\n",
    "count_vectorizer_ng = CountVectorizer(ngram_range=(1,2)).fit(data_4k.Text.values)\n",
    "final_ng_count = count_vectorizer_ng.transform(data_4k.Text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 127113)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ng_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train , y_test = train_test_split(final_ng_count , sentiment_4k.values, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_model = xgboost.XGBClassifier(max_depth=50,n_estimators=60,learning_rate=0.1)\n",
    "xgb_model.fit(X_train,y_train)\n",
    "predictions = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.80      0.78       559\n",
      "     postive       0.82      0.77      0.79       641\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      1200\n",
      "   macro avg       0.79      0.79      0.79      1200\n",
      "weighted avg       0.79      0.79      0.79      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,classification_report,accuracy_score\n",
    "# print('training score :'.format(f1_score(y_test,predictions)))\n",
    "\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_Score: 79.487 %\n"
     ]
    }
   ],
   "source": [
    "print('F1_Score: {:.3f} %'.format((f1_score(y_test,predictions,pos_label='postive'))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[448 111]\n",
      " [145 496]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-gram Level TF-IDF + Xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word',ngram_range=(2,3))\n",
    "tfidf_vectorizer.fit(data_4k.Text)\n",
    "final_tfidf_ng = tfidf_vectorizer.transform(data_4k.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train , y_test = train_test_split(final_tfidf_ng , sentiment_4k, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgboost.XGBClassifier(max_depth=50,n_estimators=60,learning_rate=0.01)\n",
    "xgb_model.fit(X_train,y_train)\n",
    "predictions = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_Score: 47.351 %\n"
     ]
    }
   ],
   "source": [
    "print('F1_Score: {:.3f} %'.format((f1_score(y_test,predictions,pos_label='postive'))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[523  78]\n",
      " [389 210]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative']\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "print(xgb_model.predict(tfidf_vectorizer.transform(['The food is not at all tasty. I won\\'t come again'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative']\n"
     ]
    }
   ],
   "source": [
    "print(xgb_model.predict(tfidf_vectorizer.transform(['Very Tasty'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative']\n"
     ]
    }
   ],
   "source": [
    "print(xgb_model.predict(tfidf_vectorizer.transform(['Hate the food'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character Level TF-IDF +Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer='char',ngram_range=(2,3))\n",
    "tfidf_vectorizer.fit(data_4k.Text)\n",
    "final_tfidf_ng = tfidf_vectorizer.transform(data_4k.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train , y_test = train_test_split(final_tfidf_ng , sentiment_4k, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgboost.XGBClassifier(max_depth=50,n_estimators=60,learning_rate=0.1)\n",
    "xgb_model.fit(X_train,y_train)\n",
    "predictions = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_Score: 76.105 %\n"
     ]
    }
   ],
   "source": [
    "print('F1_Score: {:.3f} %'.format((f1_score(y_test,predictions,pos_label='postive'))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['postive']\n"
     ]
    }
   ],
   "source": [
    "print(xgb_model.predict(tfidf_vectorizer.transform(['The food was good nice and Tasty'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative']\n"
     ]
    }
   ],
   "source": [
    "print(xgb_model.predict(tfidf_vectorizer.transform(['Unlike the food and was Not Tasty'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally here we see that with Character level TF-IDF , we see some correct classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.74      0.75       596\n",
      "     postive       0.75      0.77      0.76       604\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      1200\n",
      "   macro avg       0.76      0.76      0.76      1200\n",
      "weighted avg       0.76      0.76      0.76      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As next steps, I would implement Word2Vec Embeddings to find the dissimilarity between the words "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
