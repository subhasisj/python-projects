{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now implement the LSTM Model for training the NMT with the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000 # max no. of words for tokenizer , Top 5000 Words in the Vocabulary\n",
    "MAX_SEQUENCE_LENGTH = 200 # max length of each entry (sentence)\n",
    "EMBEDDING_DIM = 300      # embedding dimensions for word vectors\n",
    "OOV_TOKEN = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  source   target                                           comments  \\\n0    Go.      Ve.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...   \n1    Go.    Vete.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...   \n2    Go.    Vaya.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...   \n3    Go.  Váyase.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...   \n4    Hi.    Hola.  CC-BY 2.0 (France) Attribution: tatoeba.org #5...   \n\n  cleaned_source      cleaned_target  \n0             go      START_ Ve _END  \n1             go    START_ Vete _END  \n2             go    START_ Vaya _END  \n3             go  START_ Váyase _END  \n4             hi    START_ Hola _END  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>target</th>\n      <th>comments</th>\n      <th>cleaned_source</th>\n      <th>cleaned_target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Go.</td>\n      <td>Ve.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n      <td>go</td>\n      <td>START_ Ve _END</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Go.</td>\n      <td>Vete.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n      <td>go</td>\n      <td>START_ Vete _END</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Go.</td>\n      <td>Vaya.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n      <td>go</td>\n      <td>START_ Vaya _END</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Go.</td>\n      <td>Váyase.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n      <td>go</td>\n      <td>START_ Váyase _END</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Hi.</td>\n      <td>Hola.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n      <td>hi</td>\n      <td>START_ Hola _END</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Read Data\n",
    "df = pd.read_csv('./Data/cleaned_data.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us now convert the sentences into number sequences and  pad the sequences with Zeros to make all the inputs of equal size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the maximum words in the sample\n",
    "max_en_words_per_sample = max([len(sample.split()) for sample in df.cleaned_source])\n",
    "max_es_words_per_sample = max([len(sample.split()) for sample in df.cleaned_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Maximum EN words in sample: 43\nMaximum ES words in sample: 51\n"
    }
   ],
   "source": [
    "print(f'Maximum EN words in sample: {max_en_words_per_sample}')\n",
    "print(f'Maximum ES words in sample: {max_es_words_per_sample}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word2index and index2word\n",
    "def vocab_creator(texts,vocab_size = VOCAB_SIZE):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    # sequences = tokenizer.texts_to_sequences(texts)\n",
    "    # sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences,\n",
    "    # maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "    word_index_dictionary = tokenizer.word_index\n",
    "\n",
    "    word2index = {}\n",
    "    index2word = {}\n",
    "\n",
    "    for key,value in word_index_dictionary.items():\n",
    "        if value < VOCAB_SIZE:\n",
    "            word2index[key] = value\n",
    "            index2word[value] = key\n",
    "        if value >= VOCAB_SIZE-1:\n",
    "             continue\n",
    "    return word2index,index2word\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_word2index ,source_index2word = vocab_creator(df.cleaned_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{1: 'be',\n 2: 'the',\n 3: 'to',\n 4: 'tom',\n 5: 'do',\n 6: 'a',\n 7: \"n't\",\n 8: 'have',\n 9: \"'s\",\n 10: 'that',\n 11: 'in',\n 12: 'of',\n 13: 'this',\n 14: 'go',\n 15: 'for'}"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "dict(list(source_index2word.items())[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word2index ,target_index2word = vocab_creator(df.cleaned_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'start': 1,\n 'end': 2,\n 'de': 3,\n 'que': 4,\n 'no': 5,\n 'a': 6,\n 'tom': 7,\n 'la': 8,\n '¿': 9,\n 'el': 10,\n 'en': 11,\n 'es': 12,\n 'un': 13,\n 'se': 14,\n 'por': 15}"
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "dict(list(target_word2index.items())[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((111393,), (12377,))"
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.cleaned_source, df.cleaned_target, test_size = 0.1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Glove vector\n",
    "EMBEDDING_DIM = 300      # embedding dimensions for word vectors\n",
    "GLOVE_DIR = f\"../GloVe/glove.42B.{EMBEDDING_DIM}d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_index(glove_dir): \n",
    "    \n",
    "    embeddings_index = {}\n",
    "    f = open(glove_dir,encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        embeddings_index[word] = np.asarray(values[1:],dtype='float32')\n",
    "    f.close()\n",
    "\n",
    "    return embeddings_index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = create_embeddings_index(GLOVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embeddings_matrix = np.zeros((len(source_word2index)+1,EMBEDDING_DIM))\n",
    "for word,i in source_word2index.items():\n",
    "    embeddings_vector = embeddings_index.get(word)\n",
    "    if embeddings_vector is not None:\n",
    "        embeddings_matrix[i] = embeddings_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to create the encoder and decoder inputs as a generator which will zero pad all sequences and make them of the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(x,y,batch_size=128):\n",
    "    while True:\n",
    "        for current_batch_index in range(0,len(x),batch_size):\n",
    "            encoder_data_input = np.zeros((batch_size,max_en_words_per_sample),dtype='float32')\n",
    "            decoder_data_input = np.zeros((batch_size,max_es_words_per_sample),dtype='float32')\n",
    "            decoder_target_input = np.zeros((batch_size,max_es_words_per_sample,len(target_word2index)+1 ),dtype='float32')\n",
    "\n",
    "            for row_index ,(input_text, target_text) in enumerate(zip(x[current_batch_index:current_batch_index+batch_size], y[current_batch_index:current_batch_index+batch_size])):\n",
    "                for word_index, word in enumerate(input_text.split()):\n",
    "                    encoder_data_input[row_index,word_index] = source_word2index[word]\n",
    "                for word_index, word in enumerate(target_text.split()):\n",
    "                    decoder_data_input[row_index,word_index] = target_word2index[word] \n",
    "                    if word_index > 0:\n",
    "                        decoder_target_input[row_index,word_index-1,target_word2index[word]] = 1\n",
    "    yield ([encoder_data_input,decoder_data_input],decoder_target_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create a generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 50 # Hidden layers dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_inputs = tf.keras.layers.Input(shape=(max_en_words_per_sample, ),name=\"encoder_inputs\", dtype='int32')\n",
    "\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(None, ),name=\"encoder_inputs\", dtype='int32')\n",
    "\n",
    "# Hidden layers of the encoder :\n",
    "embedding_layer = tf.keras.layers.Embedding(len(source_word2index)+1,\n",
    "                                            output_dim=EMBEDDING_DIM,        \n",
    "                                            weights = [embeddings_matrix],\n",
    "                                            # input_length=max_en_words_per_sample,\n",
    "                                            trainable=False,\n",
    "                                            name = 'embeddings' )\n",
    "\n",
    "embedded_inputs = embedding_layer(encoder_inputs)\n",
    "encoder_LSTM = tf.keras.layers.LSTM(latent_dim,return_state=True) \n",
    "\n",
    "# Output layer of the encoder :\n",
    "# encoder_LSTM2_Layer = tf.keras.layers.LSTM(1024,return_sequences=True)\n",
    "\n",
    "encoder_ouputs,state_h,state_c = encoder_LSTM(embedded_inputs)\n",
    "\n",
    "# Next we discard the Encoder output and only keep the states\n",
    "encoder_states =  [state_h,state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the decoder\n",
    "# Input layer of the decoder :\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "emb_layer_decoder = tf.keras.layers.Embedding(len(target_word2index)+1,latent_dim, mask_zero=True)(decoder_inputs)\n",
    "\n",
    "decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(emb_layer_decoder, initial_state=encoder_states)\n",
    "\n",
    "# Use a softmax to generate a probability distribution over the target vocabulary for each time step\n",
    "\n",
    "decoder_dense = tf.keras.layers.Dense(len(target_word2index)+1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}