{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now implement the LSTM Model for training the NMT with the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000 # max no. of words for tokenizer , Top 5000 Words in the Vocabulary\n",
    "MAX_SEQUENCE_LENGTH = 200 # max length of each entry (sentence)\n",
    "EMBEDDING_DIM = 300      # embedding dimensions for word vectors\n",
    "OOV_TOKEN = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "df = pd.read_csv('./Data/cleaned_data.csv')\n",
    "df = df.head(10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us now convert the sentences into number sequences and  pad the sequences with Zeros to make all the inputs of equal size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the maximum words in the sample\n",
    "max_en_words_per_sample = max([len(sample.split()) for sample in df.cleaned_source])\n",
    "max_es_words_per_sample = max([len(sample.split()) for sample in df.cleaned_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Maximum EN words in sample: 5\nMaximum ES words in sample: 10\n"
    }
   ],
   "source": [
    "print(f'Maximum EN words in sample: {max_en_words_per_sample}')\n",
    "print(f'Maximum ES words in sample: {max_es_words_per_sample}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word2index and index2word\n",
    "# def vocab_creator(texts,vocab_size = VOCAB_SIZE):\n",
    "#     tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE)\n",
    "#     tokenizer.fit_on_texts(texts)\n",
    "#     # sequences = tokenizer.texts_to_sequences(texts)\n",
    "#     # sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences,\n",
    "#     # maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "#     word_index_dictionary = tokenizer.word_index\n",
    "\n",
    "#     word2index = {}\n",
    "#     index2word = {}\n",
    "\n",
    "#     for key,value in word_index_dictionary.items():\n",
    "#         if value < VOCAB_SIZE:\n",
    "#             word2index[key] = value\n",
    "#             index2word[value] = key\n",
    "#         if value >= VOCAB_SIZE-1:\n",
    "#              continue\n",
    "#     return word2index,index2word\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find all the source and target words and sort them\n",
    "# Vocabulary of Source language\n",
    "all_source_words=set()\n",
    "for source in df.cleaned_source:\n",
    "    for word in source.split():\n",
    "        if word not in all_source_words:\n",
    "            all_source_words.add(word)\n",
    "# Vocabulary of Target \n",
    "all_target_words=set()\n",
    "for target in df.cleaned_target:\n",
    "    for word in target.split():\n",
    "        if word not in all_target_words:\n",
    "            all_target_words.add(word)\n",
    "# sort all unique source and target words\n",
    "\n",
    "source_words= sorted(list(all_source_words))\n",
    "target_words=sorted(list(all_target_words))\n",
    "\n",
    "num_source_tokens = len(source_words)\n",
    "num_target_tokens = len(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# creating a word to index(word2idx) for source and target\n",
    "source_word2index= dict([(word, i+1) for i,word in enumerate(source_words)])\n",
    "target_word2index=dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "\n",
    "#creating a dictionary for index to word for source and target vocabulary\n",
    "source_index2word= dict([(i, word) for word, i in  source_word2index.items()])\n",
    "target_index2word =dict([(i, word) for word, i in target_word2index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_word2index ,source_index2word = vocab_creator(df.cleaned_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{1: 'a',\n 2: 'abandon',\n 3: 'abducted',\n 4: 'able',\n 5: 'aboard',\n 6: 'about',\n 7: 'above',\n 8: 'abroad',\n 9: 'absent',\n 10: 'absurd',\n 11: 'accelerated',\n 12: 'accept',\n 13: 'ache',\n 14: 'ached',\n 15: 'aches'}"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "dict(list(source_index2word.items())[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_word2index ,target_index2word = vocab_creator(df.cleaned_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'START_': 1,\n '_END': 2,\n 'a': 3,\n 'aabe': 4,\n 'abajo': 5,\n 'abandona': 6,\n 'abandonaron': 7,\n 'abandonen': 8,\n 'abandono': 9,\n 'abatio': 10,\n 'abejas': 11,\n 'abierta': 12,\n 'abofeteo': 13,\n 'abogado': 14,\n 'abogados': 15}"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "dict(list(target_word2index.items())[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_word2index['Â¿']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((9000,), (1000,))"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.cleaned_source, df.cleaned_target, test_size = 0.1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Glove vector\n",
    "EMBEDDING_DIM = 300      # embedding dimensions for word vectors\n",
    "GLOVE_DIR = f\"../GloVe/glove.42B.{EMBEDDING_DIM}d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_index(glove_dir): \n",
    "    \n",
    "    embeddings_index = {}\n",
    "    f = open(glove_dir,encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        embeddings_index[word] = np.asarray(values[1:],dtype='float32')\n",
    "    f.close()\n",
    "\n",
    "    return embeddings_index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = create_embeddings_index(GLOVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embeddings_matrix = np.zeros((num_source_tokens+1,EMBEDDING_DIM))\n",
    "for word,i in source_word2index.items():\n",
    "    embeddings_vector = embeddings_index.get(word)\n",
    "    if embeddings_vector is not None:\n",
    "        embeddings_matrix[i] = embeddings_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add START_ and END_ to tokenized dictionary\n",
    "# target_word2index.pop('start')\n",
    "# target_word2index.pop('end')\n",
    "\n",
    "# target_word2index['START_']  = 1\n",
    "# target_word2index['_END']  = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to create the encoder and decoder inputs as a generator which will zero pad all sequences and make them of the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_target_tokens +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_en_words_per_sample),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_es_words_per_sample),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_es_words_per_sample, num_target_tokens),dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                   \n",
    "                    encoder_input_data[i, t] = source_word2index[word] \n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_word2index[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        #print(word)\n",
    "                        decoder_target_data[i, t - 1, target_word2index[word]] = 1.\n",
    "                    \n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def generate_batch(x,y,batch_size=128):\n",
    "    while True:\n",
    "        for current_batch_index in range(0,len(x),batch_size):\n",
    "            encoder_data_input = np.zeros((batch_size,max_en_words_per_sample),dtype='float32')\n",
    "            decoder_data_input = np.zeros((batch_size,max_es_words_per_sample),dtype='float32')\n",
    "            decoder_target_input = np.zeros((batch_size,max_es_words_per_sample,len(target_word2index)+1 ),dtype='float32')\n",
    "\n",
    "            for row_index ,(input_text, target_text) in enumerate(zip(x[current_batch_index:current_batch_index+batch_size], y[current_batch_index:current_batch_index+batch_size])):\n",
    "                for word_index, word in enumerate(input_text.split()):\n",
    "                    encoder_data_input[row_index,word_index] = source_word2index[word]\n",
    "                for word_index, word in enumerate(target_text.split()):\n",
    "                    if word_index < (len(target_text.split())) - 1:\n",
    "                        # print(word_index)\n",
    "                        # print(word)\n",
    "                        decoder_data_input[row_index,word_index] = target_word2index[word] \n",
    "                    if word_index > 0:\n",
    "                        decoder_target_input[row_index,word_index-1,target_word2index[word]] = 1\n",
    "    yield ([encoder_data_input,decoder_data_input],decoder_target_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create a generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 50 # Hidden layers dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_inputs = tf.keras.layers.Input(shape=(max_en_words_per_sample, ),name=\"encoder_inputs\", dtype='int32')\n",
    "\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(None, ),name=\"encoder_inputs\", dtype='int32')\n",
    "\n",
    "# Hidden layers of the encoder :\n",
    "embedding_layer = tf.keras.layers.Embedding(num_source_tokens+1,\n",
    "                                            output_dim=EMBEDDING_DIM,        \n",
    "                                            weights = [embeddings_matrix],\n",
    "                                            # input_length=max_en_words_per_sample,\n",
    "                                            trainable=False,\n",
    "                                            name = 'embeddings',mask_zero=True )\n",
    "# embedding_layer =  tf.keras.layers.Embedding(num_source_tokens+1, latent_dim, mask_zero = True)\n",
    "\n",
    "embedded_inputs = embedding_layer(encoder_inputs)\n",
    "encoder_LSTM = tf.keras.layers.LSTM(latent_dim,return_state=True) \n",
    "\n",
    "# Output layer of the encoder :\n",
    "# encoder_LSTM2_Layer = tf.keras.layers.LSTM(1024,return_sequences=True)\n",
    "\n",
    "encoder_ouputs,state_h,state_c = encoder_LSTM(embedded_inputs)\n",
    "\n",
    "# Next we discard the Encoder output and only keep the states\n",
    "encoder_states =  [state_h,state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nencoder_inputs (InputLayer)     [(None, None)]       0                                            \n__________________________________________________________________________________________________\ninput_1 (InputLayer)            [(None, None)]       0                                            \n__________________________________________________________________________________________________\nembeddings (Embedding)          (None, None, 300)    686400      encoder_inputs[0][0]             \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, None, 50)     227100      input_1[0][0]                    \n__________________________________________________________________________________________________\nlstm (LSTM)                     [(None, 50), (None,  70200       embeddings[0][0]                 \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   [(None, None, 50), ( 20200       embedding[0][0]                  \n                                                                 lstm[0][1]                       \n                                                                 lstm[0][2]                       \n__________________________________________________________________________________________________\ndense (Dense)                   (None, None, 4542)   231642      lstm_1[0][0]                     \n==================================================================================================\nTotal params: 1,235,542\nTrainable params: 549,142\nNon-trainable params: 686,400\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "# Building the decoder\n",
    "# Input layer of the decoder :\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the   `\n",
    "# return states in the training model, but we will use them in inference.\n",
    "emb_layer_decoder = tf.keras.layers.Embedding(num_target_tokens,latent_dim, mask_zero=True)(decoder_inputs)\n",
    "\n",
    "decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(emb_layer_decoder, initial_state=encoder_states)\n",
    "\n",
    "# Use a softmax to generate a probability distribution over the target vocabulary for each time step\n",
    "\n",
    "decoder_dense = tf.keras.layers.Dense(num_target_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Checkpoint\n",
    "import os\n",
    "checkpoint_name = os.path.join('checkpoints','Weights-{epoch:03d}--{val_loss:.5f}.hdf5')\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_weights_only = True, mode ='auto')\n",
    "\n",
    "import datetime\n",
    "log_dir = \"logs\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)\n",
    "\n",
    "callbacks_list = [checkpoint,tensorboard_callback]\n",
    "# callbacks_list = [tensorboard_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n"
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Reusing TensorBoard on port 6006 (pid 43208), started 1:04:32 ago. (Use '!kill 43208' to kill it.)"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logs_base_dir = \"logs\"\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "%tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[]\n"
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "# for gpu in gpus:\n",
    "#         tf.config.experimental.set_memory_growth(gpu, True)\n",
    "# tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nTrain for 70 steps, validate for 7 steps\nEpoch 1/40\n69/70 [============================>.] - ETA: 0s - loss: 1.7441 - acc: 0.3205\nEpoch 00001: saving model to checkpoints\\Weights-001--1.84121.hdf5\n70/70 [==============================] - 22s 311ms/step - loss: 1.7440 - acc: 0.3208 - val_loss: 1.8412 - val_acc: 0.3304\nEpoch 2/40\n69/70 [============================>.] - ETA: 0s - loss: 1.6540 - acc: 0.3387\nEpoch 00002: saving model to checkpoints\\Weights-002--1.79353.hdf5\n70/70 [==============================] - 23s 325ms/step - loss: 1.6541 - acc: 0.3387 - val_loss: 1.7935 - val_acc: 0.3419\nEpoch 3/40\n69/70 [============================>.] - ETA: 0s - loss: 1.5894 - acc: 0.3546\nEpoch 00003: saving model to checkpoints\\Weights-003--1.74965.hdf5\n70/70 [==============================] - 21s 307ms/step - loss: 1.5912 - acc: 0.3545 - val_loss: 1.7497 - val_acc: 0.3557\nEpoch 4/40\n69/70 [============================>.] - ETA: 0s - loss: 1.5352 - acc: 0.3698\nEpoch 00004: saving model to checkpoints\\Weights-004--1.71138.hdf5\n70/70 [==============================] - 23s 332ms/step - loss: 1.5339 - acc: 0.3701 - val_loss: 1.7114 - val_acc: 0.3707\nEpoch 5/40\n69/70 [============================>.] - ETA: 0s - loss: 1.4881 - acc: 0.3858\nEpoch 00005: saving model to checkpoints\\Weights-005--1.69044.hdf5\n70/70 [==============================] - 25s 356ms/step - loss: 1.4871 - acc: 0.3865 - val_loss: 1.6904 - val_acc: 0.3779\nEpoch 6/40\n69/70 [============================>.] - ETA: 0s - loss: 1.4423 - acc: 0.4009\nEpoch 00006: saving model to checkpoints\\Weights-006--1.64179.hdf5\n70/70 [==============================] - 22s 318ms/step - loss: 1.4420 - acc: 0.4009 - val_loss: 1.6418 - val_acc: 0.3964\nEpoch 7/40\n69/70 [============================>.] - ETA: 0s - loss: 1.4009 - acc: 0.4149\nEpoch 00007: saving model to checkpoints\\Weights-007--1.62202.hdf5\n70/70 [==============================] - 23s 322ms/step - loss: 1.4004 - acc: 0.4153 - val_loss: 1.6220 - val_acc: 0.4056\nEpoch 8/40\n69/70 [============================>.] - ETA: 0s - loss: 1.3632 - acc: 0.4283\nEpoch 00008: saving model to checkpoints\\Weights-008--1.59238.hdf5\n70/70 [==============================] - 20s 283ms/step - loss: 1.3626 - acc: 0.4284 - val_loss: 1.5924 - val_acc: 0.4142\nEpoch 9/40\n69/70 [============================>.] - ETA: 0s - loss: 1.3278 - acc: 0.4399\nEpoch 00009: saving model to checkpoints\\Weights-009--1.56831.hdf5\n70/70 [==============================] - 21s 304ms/step - loss: 1.3273 - acc: 0.4399 - val_loss: 1.5683 - val_acc: 0.4281\nEpoch 10/40\n69/70 [============================>.] - ETA: 0s - loss: 1.2940 - acc: 0.4508\nEpoch 00010: saving model to checkpoints\\Weights-010--1.54495.hdf5\n70/70 [==============================] - 21s 294ms/step - loss: 1.2947 - acc: 0.4507 - val_loss: 1.5449 - val_acc: 0.4347\nEpoch 11/40\n69/70 [============================>.] - ETA: 0s - loss: 1.2633 - acc: 0.4582\nEpoch 00011: saving model to checkpoints\\Weights-011--1.53342.hdf5\n70/70 [==============================] - 22s 313ms/step - loss: 1.2632 - acc: 0.4585 - val_loss: 1.5334 - val_acc: 0.4393\nEpoch 12/40\n69/70 [============================>.] - ETA: 0s - loss: 1.2350 - acc: 0.4658\nEpoch 00012: saving model to checkpoints\\Weights-012--1.50719.hdf5\n70/70 [==============================] - 22s 311ms/step - loss: 1.2354 - acc: 0.4657 - val_loss: 1.5072 - val_acc: 0.4468\nEpoch 13/40\n69/70 [============================>.] - ETA: 0s - loss: 1.2091 - acc: 0.4724\nEpoch 00013: saving model to checkpoints\\Weights-013--1.49672.hdf5\n70/70 [==============================] - 23s 325ms/step - loss: 1.2082 - acc: 0.4727 - val_loss: 1.4967 - val_acc: 0.4485\nEpoch 14/40\n69/70 [============================>.] - ETA: 0s - loss: 1.1839 - acc: 0.4783\nEpoch 00014: saving model to checkpoints\\Weights-014--1.48105.hdf5\n70/70 [==============================] - 22s 314ms/step - loss: 1.1838 - acc: 0.4783 - val_loss: 1.4810 - val_acc: 0.4546\nEpoch 15/40\n69/70 [============================>.] - ETA: 0s - loss: 1.1590 - acc: 0.4857\nEpoch 00015: saving model to checkpoints\\Weights-015--1.46917.hdf5\n70/70 [==============================] - 22s 313ms/step - loss: 1.1591 - acc: 0.4854 - val_loss: 1.4692 - val_acc: 0.4583\nEpoch 16/40\n69/70 [============================>.] - ETA: 0s - loss: 1.1361 - acc: 0.4931\nEpoch 00016: saving model to checkpoints\\Weights-016--1.45864.hdf5\n70/70 [==============================] - 23s 326ms/step - loss: 1.1356 - acc: 0.4934 - val_loss: 1.4586 - val_acc: 0.4609\nEpoch 17/40\n69/70 [============================>.] - ETA: 0s - loss: 1.1131 - acc: 0.4980\nEpoch 00017: saving model to checkpoints\\Weights-017--1.44488.hdf5\n70/70 [==============================] - 22s 315ms/step - loss: 1.1141 - acc: 0.4977 - val_loss: 1.4449 - val_acc: 0.4644\nEpoch 18/40\n69/70 [============================>.] - ETA: 0s - loss: 1.0907 - acc: 0.5048\nEpoch 00018: saving model to checkpoints\\Weights-018--1.44315.hdf5\n70/70 [==============================] - 20s 282ms/step - loss: 1.0908 - acc: 0.5046 - val_loss: 1.4431 - val_acc: 0.4653\nEpoch 19/40\n69/70 [============================>.] - ETA: 0s - loss: 1.0727 - acc: 0.5093\nEpoch 00019: saving model to checkpoints\\Weights-019--1.42774.hdf5\n70/70 [==============================] - 20s 287ms/step - loss: 1.0717 - acc: 0.5095 - val_loss: 1.4277 - val_acc: 0.4661\nEpoch 20/40\n69/70 [============================>.] - ETA: 0s - loss: 1.0522 - acc: 0.5145\nEpoch 00020: saving model to checkpoints\\Weights-020--1.41844.hdf5\n70/70 [==============================] - 21s 305ms/step - loss: 1.0529 - acc: 0.5144 - val_loss: 1.4184 - val_acc: 0.4687\nEpoch 21/40\n69/70 [============================>.] - ETA: 0s - loss: 1.0317 - acc: 0.5200\nEpoch 00021: saving model to checkpoints\\Weights-021--1.40917.hdf5\n70/70 [==============================] - 22s 310ms/step - loss: 1.0319 - acc: 0.5201 - val_loss: 1.4092 - val_acc: 0.4713\nEpoch 22/40\n69/70 [============================>.] - ETA: 0s - loss: 1.0141 - acc: 0.5249\nEpoch 00022: saving model to checkpoints\\Weights-022--1.40596.hdf5\n70/70 [==============================] - 21s 298ms/step - loss: 1.0141 - acc: 0.5249 - val_loss: 1.4060 - val_acc: 0.4739\nEpoch 23/40\n69/70 [============================>.] - ETA: 0s - loss: 0.9965 - acc: 0.5302\nEpoch 00023: saving model to checkpoints\\Weights-023--1.39913.hdf5\n70/70 [==============================] - 23s 323ms/step - loss: 0.9965 - acc: 0.5301 - val_loss: 1.3991 - val_acc: 0.4779\nEpoch 24/40\n69/70 [============================>.] - ETA: 0s - loss: 0.9791 - acc: 0.5350\nEpoch 00024: saving model to checkpoints\\Weights-024--1.39184.hdf5\n70/70 [==============================] - 21s 305ms/step - loss: 0.9792 - acc: 0.5351 - val_loss: 1.3918 - val_acc: 0.4831\nEpoch 25/40\n69/70 [============================>.] - ETA: 0s - loss: 0.9610 - acc: 0.5406\nEpoch 00025: saving model to checkpoints\\Weights-025--1.38425.hdf5\n70/70 [==============================] - 21s 307ms/step - loss: 0.9621 - acc: 0.5407 - val_loss: 1.3842 - val_acc: 0.4788\nEpoch 26/40\n69/70 [============================>.] - ETA: 0s - loss: 0.9450 - acc: 0.5463\nEpoch 00026: saving model to checkpoints\\Weights-026--1.38548.hdf5\n70/70 [==============================] - 22s 319ms/step - loss: 0.9449 - acc: 0.5463 - val_loss: 1.3855 - val_acc: 0.4777\nEpoch 27/40\n69/70 [============================>.] - ETA: 0s - loss: 0.9298 - acc: 0.5526\nEpoch 00027: saving model to checkpoints\\Weights-027--1.37811.hdf5\n70/70 [==============================] - 21s 300ms/step - loss: 0.9303 - acc: 0.5522 - val_loss: 1.3781 - val_acc: 0.4834\nEpoch 28/40\n69/70 [============================>.] - ETA: 0s - loss: 0.9133 - acc: 0.5578\nEpoch 00028: saving model to checkpoints\\Weights-028--1.37397.hdf5\n70/70 [==============================] - 21s 306ms/step - loss: 0.9135 - acc: 0.5580 - val_loss: 1.3740 - val_acc: 0.4880\nEpoch 29/40\n69/70 [============================>.] - ETA: 0s - loss: 0.8988 - acc: 0.5646\nEpoch 00029: saving model to checkpoints\\Weights-029--1.36588.hdf5\n70/70 [==============================] - 21s 304ms/step - loss: 0.8988 - acc: 0.5644 - val_loss: 1.3659 - val_acc: 0.4898\nEpoch 30/40\n69/70 [============================>.] - ETA: 0s - loss: 0.8848 - acc: 0.5699\nEpoch 00030: saving model to checkpoints\\Weights-030--1.36062.hdf5\n70/70 [==============================] - 23s 322ms/step - loss: 0.8841 - acc: 0.5700 - val_loss: 1.3606 - val_acc: 0.4926\nEpoch 31/40\n69/70 [============================>.] - ETA: 0s - loss: 0.8703 - acc: 0.5748\nEpoch 00031: saving model to checkpoints\\Weights-031--1.35746.hdf5\n70/70 [==============================] - 23s 324ms/step - loss: 0.8708 - acc: 0.5745 - val_loss: 1.3575 - val_acc: 0.4924\nEpoch 32/40\n69/70 [============================>.] - ETA: 0s - loss: 0.8561 - acc: 0.5809\nEpoch 00032: saving model to checkpoints\\Weights-032--1.35772.hdf5\n70/70 [==============================] - 24s 344ms/step - loss: 0.8556 - acc: 0.5811 - val_loss: 1.3577 - val_acc: 0.4926\nEpoch 33/40\n69/70 [============================>.] - ETA: 0s - loss: 0.8436 - acc: 0.5855\nEpoch 00033: saving model to checkpoints\\Weights-033--1.35507.hdf5\n70/70 [==============================] - 25s 354ms/step - loss: 0.8432 - acc: 0.5857 - val_loss: 1.3551 - val_acc: 0.4883\nEpoch 34/40\n69/70 [============================>.] - ETA: 0s - loss: 0.8295 - acc: 0.5911\nEpoch 00034: saving model to checkpoints\\Weights-034--1.35597.hdf5\n70/70 [==============================] - 21s 300ms/step - loss: 0.8296 - acc: 0.5912 - val_loss: 1.3560 - val_acc: 0.4883\nEpoch 35/40\n69/70 [============================>.] - ETA: 0s - loss: 0.8171 - acc: 0.5960\nEpoch 00035: saving model to checkpoints\\Weights-035--1.34704.hdf5\n70/70 [==============================] - 22s 310ms/step - loss: 0.8170 - acc: 0.5960 - val_loss: 1.3470 - val_acc: 0.4926\nEpoch 36/40\n69/70 [============================>.] - ETA: 0s - loss: 0.8048 - acc: 0.6013\nEpoch 00036: saving model to checkpoints\\Weights-036--1.34702.hdf5\n70/70 [==============================] - 22s 315ms/step - loss: 0.8042 - acc: 0.6014 - val_loss: 1.3470 - val_acc: 0.4929\nEpoch 37/40\n69/70 [============================>.] - ETA: 0s - loss: 0.7927 - acc: 0.6065\nEpoch 00037: saving model to checkpoints\\Weights-037--1.34436.hdf5\n70/70 [==============================] - 21s 299ms/step - loss: 0.7932 - acc: 0.6064 - val_loss: 1.3444 - val_acc: 0.4955\nEpoch 38/40\n69/70 [============================>.] - ETA: 0s - loss: 0.7799 - acc: 0.6099\nEpoch 00038: saving model to checkpoints\\Weights-038--1.33992.hdf5\n70/70 [==============================] - 21s 302ms/step - loss: 0.7798 - acc: 0.6098 - val_loss: 1.3399 - val_acc: 0.4973\nEpoch 39/40\n69/70 [============================>.] - ETA: 0s - loss: 0.7693 - acc: 0.6152\nEpoch 00039: saving model to checkpoints\\Weights-039--1.34454.hdf5\n70/70 [==============================] - 21s 297ms/step - loss: 0.7691 - acc: 0.6153 - val_loss: 1.3445 - val_acc: 0.4950\nEpoch 40/40\n69/70 [============================>.] - ETA: 0s - loss: 0.7577 - acc: 0.6199\nEpoch 00040: saving model to checkpoints\\Weights-040--1.33494.hdf5\n70/70 [==============================] - 24s 336ms/step - loss: 0.7576 - acc: 0.6198 - val_loss: 1.3349 - val_acc: 0.4964\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x251e29eb588>"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "# start training\n",
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 128\n",
    "model.fit(generate_batch(X_train,y_train,batch_size=batch_size),\n",
    "        epochs = 40,\n",
    "        steps_per_epoch = train_samples//batch_size,\n",
    "        validation_data=generate_batch(X_test,y_test,batch_size = batch_size),\n",
    "        validation_steps= val_samples // batch_size,\n",
    "        verbose =1,\n",
    "        callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('weights'):\n",
    "    os.makedirs('weights')\n",
    "model.save_weights(os.path.join('weights','seq2seq_translation_weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(os.path.join('weights','seq2seq_translation_weights.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = tf.keras.models.Model(encoder_inputs,encoder_states)\n",
    "\n",
    "# Decoder Setup\n",
    "decoder_state_input_h = tf.keras.layers.Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = tf.keras.layers.Input(shape=(latent_dim,))\n",
    "decoder_states_input = [decoder_state_input_h,decoder_state_input_c]\n",
    "\n",
    "# inference_dec_emb_layer = emb_layer_decoder(decoder_inputs)\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs_2,state_h2,state_c2 = decoder_lstm(emb_layer_decoder,\n",
    "                                                    initial_state=decoder_states_input)\n",
    "\n",
    "decoder_states2 = [state_h2,state_c2]\n",
    "decoder_outputs_2 = decoder_dense(decoder_outputs_2)\n",
    "\n",
    "# Final Decoder Model\n",
    "decoder_model = tf.keras.models.Model([decoder_inputs]+decoder_states_input ,\n",
    "                                        [decoder_outputs_2]+decoder_states2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_word2index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = target_index2word[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 10):\n",
    "            stop_condition = True\n",
    "        \n",
    "        # update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Input English sentence: i can read\nActual Spanish Translation:  se leer \nPredicted Spanish Translation:  yo puedo h\n====================================================================================================\nInput English sentence: stay together\nActual Spanish Translation:  no os separeis \nPredicted Spanish Translation:  quedat\n====================================================================================================\nInput English sentence: i feel at ease\nActual Spanish Translation:  me siento en paz \nPredicted Spanish Translation:  me sient\n====================================================================================================\nInput English sentence: i hate losing\nActual Spanish Translation:  detesto perder \nPredicted Spanish Translation:  odio los g\n====================================================================================================\nInput English sentence: here is my key\nActual Spanish Translation:  aqui esta mi llave \nPredicted Spanish Translation:  aqui est\n====================================================================================================\nInput English sentence: tom was skinny\nActual Spanish Translation:  tom estaba delgaducho \nPredicted Spanish Translation:  tom es\n====================================================================================================\nInput English sentence: are we broke\nActual Spanish Translation:  estamos arruinados \nPredicted Spanish Translation:  estamos est\n====================================================================================================\nInput English sentence: open the box\nActual Spanish Translation:  abra la caja \nPredicted Spanish Translation:  cierra la \n====================================================================================================\nInput English sentence: she trusts him\nActual Spanish Translation:  ella confia en el \nPredicted Spanish Translation:  ella l\n====================================================================================================\nInput English sentence: she is kind\nActual Spanish Translation:  ella es amable \nPredicted Spanish Translation:  ella e\n====================================================================================================\nInput English sentence: i want that job\nActual Spanish Translation:  quiero ese trabajo \nPredicted Spanish Translation:  quiero en\n====================================================================================================\nInput English sentence: i m too fat\nActual Spanish Translation:  yo soy demasiado gordo \nPredicted Spanish Translation:  estoy tan g\n====================================================================================================\nInput English sentence: tom is dead\nActual Spanish Translation:  tom esta muerto \nPredicted Spanish Translation:  tom esta mu\n====================================================================================================\nInput English sentence: tom is winning\nActual Spanish Translation:  tomas esta ganando \nPredicted Spanish Translation:  tom es\n====================================================================================================\nInput English sentence: this is yours\nActual Spanish Translation:  esto es tuyo \nPredicted Spanish Translation:  esto es \n====================================================================================================\nInput English sentence: you know her\nActual Spanish Translation:  tu la conoces \nPredicted Spanish Translation:  tu tu tu \n====================================================================================================\nInput English sentence: go find out\nActual Spanish Translation:  ve y descubrelo \nPredicted Spanish Translation:  ve a v\n====================================================================================================\nInput English sentence: bring food\nActual Spanish Translation:  traed comida \nPredicted Spanish Translation:  sal \n====================================================================================================\nInput English sentence: after you\nActual Spanish Translation:  despues de usted \nPredicted Spanish Translation:  te pri\n====================================================================================================\nInput English sentence: tom winked\nActual Spanish Translation:  tom hizo un guino \nPredicted Spanish Translation:  tom fue\n====================================================================================================\nInput English sentence: i m all for it\nActual Spanish Translation:  voy con todas por eso \nPredicted Spanish Translation:  no esto\n====================================================================================================\nInput English sentence: tell us more\nActual Spanish Translation:  cuentanos mas \nPredicted Spanish Translation:  mas mas\n====================================================================================================\nInput English sentence: that s personal\nActual Spanish Translation:  eso es personal \nPredicted Spanish Translation:  eso es\n====================================================================================================\nInput English sentence: it s boring\nActual Spanish Translation:  es aburrido \nPredicted Spanish Translation:  es abur\n====================================================================================================\nInput English sentence: i m not lazy\nActual Spanish Translation:  no soy perezoso \nPredicted Spanish Translation:  no soy\n====================================================================================================\nInput English sentence: i ll walk\nActual Spanish Translation:  andare \nPredicted Spanish Translation:  voy a que\n====================================================================================================\nInput English sentence: she is awkward\nActual Spanish Translation:  ella es torpe \nPredicted Spanish Translation:  ella es canadi\n====================================================================================================\nInput English sentence: tom likes beans\nActual Spanish Translation:  a tom le gustan los frijoles \nPredicted Spanish Translation:  a tom le g\n====================================================================================================\nInput English sentence: she came last\nActual Spanish Translation:  ella ha llegado la ultima \nPredicted Spanish Translation:  ella vino l\n====================================================================================================\nInput English sentence: answer me\nActual Spanish Translation:  respondanme \nPredicted Spanish Translation:  espera \n====================================================================================================\nInput English sentence: i m dying\nActual Spanish Translation:  me estoy muriendo \nPredicted Spanish Translation:  estoy de e\n====================================================================================================\nInput English sentence: are those yours\nActual Spanish Translation:  son tuyas esas \nPredicted Spanish Translation:  son son t\n====================================================================================================\nInput English sentence: it s your turn\nActual Spanish Translation:  te toca a ti \nPredicted Spanish Translation:  es tu t\n====================================================================================================\nInput English sentence: is today friday\nActual Spanish Translation:  es viernes hoy \nPredicted Spanish Translation:  es nue\n====================================================================================================\nInput English sentence: come together\nActual Spanish Translation:  vengan ambos \nPredicted Spanish Translation:  ven \n====================================================================================================\nInput English sentence: look closer\nActual Spanish Translation:  mira desde mas cerca \nPredicted Spanish Translation:  mira adel\n====================================================================================================\nInput English sentence: let s do it\nActual Spanish Translation:  hagamoslo \nPredicted Spanish Translation:  lo hac\n====================================================================================================\nInput English sentence: i m impatient\nActual Spanish Translation:  estoy impaciente \nPredicted Spanish Translation:  soy muy ti\n====================================================================================================\nInput English sentence: who will do it\nActual Spanish Translation:  quien lo hara \nPredicted Spanish Translation:  quien lo hac\n====================================================================================================\nInput English sentence: can i try it\nActual Spanish Translation:  puedo intentar \nPredicted Spanish Translation:  puedo p\n====================================================================================================\nInput English sentence: tom likes him\nActual Spanish Translation:  el le gusta a tom \nPredicted Spanish Translation:  a tom le g\n====================================================================================================\nInput English sentence: i ll arrange it\nActual Spanish Translation:  voy a arreglarlo \nPredicted Spanish Translation:  lo lo ocu\n====================================================================================================\nInput English sentence: tom got angry\nActual Spanish Translation:  tomas se enojo \nPredicted Spanish Translation:  tom se q\n====================================================================================================\nInput English sentence: just do it fast\nActual Spanish Translation:  limitate a hacerlo rapido \nPredicted Spanish Translation:  hazlo h\n====================================================================================================\nInput English sentence: you re good\nActual Spanish Translation:  eres buena \nPredicted Spanish Translation:  eres bien \n====================================================================================================\nInput English sentence: i m an agent\nActual Spanish Translation:  soy agente \nPredicted Spanish Translation:  soy un menti\n====================================================================================================\nInput English sentence: he s a slacker\nActual Spanish Translation:  el es un vago \nPredicted Spanish Translation:  el es u\n====================================================================================================\nInput English sentence: be supportive\nActual Spanish Translation:  apoyame \nPredicted Spanish Translation:  se atento \n====================================================================================================\nInput English sentence: i hate it\nActual Spanish Translation:  me la seca \nPredicted Spanish Translation:  yo lo \n====================================================================================================\nInput English sentence: i love this car\nActual Spanish Translation:  amo este auto \nPredicted Spanish Translation:  yo amo el l\n====================================================================================================\nInput English sentence: be objective\nActual Spanish Translation:  sean objetivos \nPredicted Spanish Translation:  se un \n====================================================================================================\n"
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "test_gen = generate_batch(X_test, y_test, batch_size = 1)\n",
    "Blue_score1 = []\n",
    "Blue_score2 = []\n",
    "Blue_score3 = []\n",
    "Blue_score4 = []\n",
    "\n",
    "for k in range(len(X_test)):\n",
    "  (input_seq, actual_output), _ = next(test_gen)\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "  print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "  print('Actual Spanish Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "  print('Predicted Spanish Translation:', decoded_sentence[:-4])\n",
    "  print(\"==\"*50)\n",
    "  reference = [y_test[k:k+1].values[0][6:-5].split()]\n",
    "  candidate = decoded_sentence[:-4].split()\n",
    "  score1=sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "  score2=sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0))\n",
    "  score3= sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0))\n",
    "  score4 = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "  Blue_score1.append(score1)\n",
    "  Blue_score2.append(score2)\n",
    "  Blue_score3.append(score3)\n",
    "  Blue_score4.append(score4)\n",
    "\n",
    "  if k==50:\n",
    "      break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}