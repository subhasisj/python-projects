{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now implement the LSTM Model for training the NMT with the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000 # max no. of words for tokenizer , Top 5000 Words in the Vocabulary\n",
    "MAX_SEQUENCE_LENGTH = 200 # max length of each entry (sentence)\n",
    "EMBEDDING_DIM = 300      # embedding dimensions for word vectors\n",
    "OOV_TOKEN = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>comments</th>\n",
       "      <th>cleaned_source</th>\n",
       "      <th>cleaned_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Ve.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "      <td>go</td>\n",
       "      <td>START_ ve _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vete.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "      <td>go</td>\n",
       "      <td>START_ vete _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vaya.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "      <td>go</td>\n",
       "      <td>START_ vaya _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Váyase.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "      <td>go</td>\n",
       "      <td>START_ vayase _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hola.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "      <td>hi</td>\n",
       "      <td>START_ hola _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source   target                                           comments  \\\n",
       "0    Go.      Ve.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...   \n",
       "1    Go.    Vete.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...   \n",
       "2    Go.    Vaya.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...   \n",
       "3    Go.  Váyase.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...   \n",
       "4    Hi.    Hola.  CC-BY 2.0 (France) Attribution: tatoeba.org #5...   \n",
       "\n",
       "  cleaned_source      cleaned_target  \n",
       "0             go      START_ ve _END  \n",
       "1             go    START_ vete _END  \n",
       "2             go    START_ vaya _END  \n",
       "3             go  START_ vayase _END  \n",
       "4             hi    START_ hola _END  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Data\n",
    "df = pd.read_csv('./Data/cleaned_data.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us now convert the sentences into number sequences and  pad the sequences with Zeros to make all the inputs of equal size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the maximum words in the sample\n",
    "max_en_words_per_sample = max([len(sample.split()) for sample in df.cleaned_source])\n",
    "max_es_words_per_sample = max([len(sample.split()) for sample in df.cleaned_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum EN words in sample: 47\n",
      "Maximum ES words in sample: 51\n"
     ]
    }
   ],
   "source": [
    "print(f'Maximum EN words in sample: {max_en_words_per_sample}')\n",
    "print(f'Maximum ES words in sample: {max_es_words_per_sample}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word2index and index2word\n",
    "# def vocab_creator(texts,vocab_size = VOCAB_SIZE):\n",
    "#     tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE)\n",
    "#     tokenizer.fit_on_texts(texts)\n",
    "#     # sequences = tokenizer.texts_to_sequences(texts)\n",
    "#     # sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences,\n",
    "#     # maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "#     word_index_dictionary = tokenizer.word_index\n",
    "\n",
    "#     word2index = {}\n",
    "#     index2word = {}\n",
    "\n",
    "#     for key,value in word_index_dictionary.items():\n",
    "#         if value < VOCAB_SIZE:\n",
    "#             word2index[key] = value\n",
    "#             index2word[value] = key\n",
    "#         if value >= VOCAB_SIZE-1:\n",
    "#              continue\n",
    "#     return word2index,index2word\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find all the source and target words and sort them\n",
    "# Vocabulary of Source language\n",
    "all_source_words=set()\n",
    "for source in df.cleaned_source:\n",
    "    for word in source.split():\n",
    "        if word not in all_source_words:\n",
    "            all_source_words.add(word)\n",
    "# Vocabulary of Target \n",
    "all_target_words=set()\n",
    "for target in df.cleaned_target:\n",
    "    for word in target.split():\n",
    "        if word not in all_target_words:\n",
    "            all_target_words.add(word)\n",
    "# sort all unique source and target words\n",
    "\n",
    "source_words= sorted(list(all_source_words))\n",
    "target_words=sorted(list(all_target_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# creating a word to index(word2idx) for source and target\n",
    "source_word2index= dict([(word, i+1) for i,word in enumerate(source_words)])\n",
    "target_word2index=dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "\n",
    "#creating a dictionary for index to word for source and target vocabulary\n",
    "source_index2word= dict([(i, word) for word, i in  source_word2index.items()])\n",
    "target_index2word =dict([(i, word) for word, i in target_word2index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_word2index ,source_index2word = vocab_creator(df.cleaned_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'aardvark',\n",
       " 3: 'aardvarks',\n",
       " 4: 'aaron',\n",
       " 5: 'aback',\n",
       " 6: 'abandon',\n",
       " 7: 'abandoned',\n",
       " 8: 'abandoning',\n",
       " 9: 'abate',\n",
       " 10: 'abated',\n",
       " 11: 'abating',\n",
       " 12: 'abbreviation',\n",
       " 13: 'abc',\n",
       " 14: 'abdicate',\n",
       " 15: 'abdomen'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(source_index2word.items())[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_word2index ,target_index2word = vocab_creator(df.cleaned_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'START_': 1,\n",
       " '_END': 2,\n",
       " 'a': 3,\n",
       " 'aabe': 4,\n",
       " 'aah': 5,\n",
       " 'aaron': 6,\n",
       " 'abajo': 7,\n",
       " 'abandona': 8,\n",
       " 'abandonada': 9,\n",
       " 'abandonadas': 10,\n",
       " 'abandonado': 11,\n",
       " 'abandonados': 12,\n",
       " 'abandonalo': 13,\n",
       " 'abandonamos': 14,\n",
       " 'abandonan': 15}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(target_word2index.items())[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_word2index['¿']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((111393,), (12377,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.cleaned_source, df.cleaned_target, test_size = 0.1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Glove vector\n",
    "EMBEDDING_DIM = 300      # embedding dimensions for word vectors\n",
    "GLOVE_DIR = f\"../GloVe/glove.42B.{EMBEDDING_DIM}d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_index(glove_dir): \n",
    "    \n",
    "    embeddings_index = {}\n",
    "    f = open(glove_dir,encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        embeddings_index[word] = np.asarray(values[1:],dtype='float32')\n",
    "    f.close()\n",
    "\n",
    "    return embeddings_index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_index = create_embeddings_index(GLOVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "# embeddings_matrix = np.zeros((len(source_word2index)+1,EMBEDDING_DIM))\n",
    "# for word,i in source_word2index.items():\n",
    "#     embeddings_vector = embeddings_index.get(word)\n",
    "#     if embeddings_vector is not None:\n",
    "#         embeddings_matrix[i] = embeddings_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add START_ and END_ to tokenized dictionary\n",
    "# target_word2index.pop('start')\n",
    "# target_word2index.pop('end')\n",
    "\n",
    "# target_word2index['START_']  = 1\n",
    "# target_word2index['_END']  = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to create the encoder and decoder inputs as a generator which will zero pad all sequences and make them of the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_en_words_per_sample),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_es_words_per_sample),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_es_words_per_sample, len(target_word2index)+1),dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                   \n",
    "                    encoder_input_data[i, t] = source_word2index[word] \n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_word2index[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        #print(word)\n",
    "                        decoder_target_data[i, t - 1, target_word2index[word]] = 1.\n",
    "                    \n",
    "    yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def generate_batch(x,y,batch_size=128):\n",
    "    while True:\n",
    "        for current_batch_index in range(0,len(x),batch_size):\n",
    "            encoder_data_input = np.zeros((batch_size,max_en_words_per_sample),dtype='float32')\n",
    "            decoder_data_input = np.zeros((batch_size,max_es_words_per_sample),dtype='float32')\n",
    "            decoder_target_input = np.zeros((batch_size,max_es_words_per_sample,len(target_word2index)+1 ),dtype='float32')\n",
    "\n",
    "            for row_index ,(input_text, target_text) in enumerate(zip(x[current_batch_index:current_batch_index+batch_size], y[current_batch_index:current_batch_index+batch_size])):\n",
    "                for word_index, word in enumerate(input_text.split()):\n",
    "                    encoder_data_input[row_index,word_index] = source_word2index[word]\n",
    "                for word_index, word in enumerate(target_text.split()):\n",
    "                    if word_index < (len(target_text.split())) - 1:\n",
    "                        # print(word_index)\n",
    "                        # print(word)\n",
    "                        decoder_data_input[row_index,word_index] = target_word2index[word] \n",
    "                    if word_index > 0:\n",
    "                        decoder_target_input[row_index,word_index-1,target_word2index[word]] = 1\n",
    "    yield ([encoder_data_input,decoder_data_input],decoder_target_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create a generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 50 # Hidden layers dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_inputs = tf.keras.layers.Input(shape=(max_en_words_per_sample, ),name=\"encoder_inputs\", dtype='int32')\n",
    "\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(None, ),name=\"encoder_inputs\", dtype='int32')\n",
    "\n",
    "# Hidden layers of the encoder :\n",
    "# embedding_layer = tf.keras.layers.Embedding(len(source_word2index)+1,\n",
    "#                                             output_dim=EMBEDDING_DIM,        \n",
    "#                                             weights = [embeddings_matrix],\n",
    "#                                             # input_length=max_en_words_per_sample,\n",
    "#                                             trainable=False,\n",
    "#                                             name = 'embeddings' )\n",
    "embedding_layer =  tf.keras.layers.Embedding(len(source_word2index), latent_dim, mask_zero = True)\n",
    "\n",
    "embedded_inputs = embedding_layer(encoder_inputs)\n",
    "encoder_LSTM = tf.keras.layers.LSTM(latent_dim,return_state=True) \n",
    "\n",
    "# Output layer of the encoder :\n",
    "# encoder_LSTM2_Layer = tf.keras.layers.LSTM(1024,return_sequences=True)\n",
    "\n",
    "encoder_ouputs,state_h,state_c = encoder_LSTM(embedded_inputs)\n",
    "\n",
    "# Next we discard the Encoder output and only keep the states\n",
    "encoder_states =  [state_h,state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the decoder\n",
    "# Input layer of the decoder :\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the   `\n",
    "# return states in the training model, but we will use them in inference.\n",
    "emb_layer_decoder = tf.keras.layers.Embedding(len(target_word2index)+1,latent_dim, mask_zero=True)(decoder_inputs)\n",
    "\n",
    "decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(emb_layer_decoder, initial_state=encoder_states)\n",
    "\n",
    "# Use a softmax to generate a probability distribution over the target vocabulary for each time step\n",
    "\n",
    "decoder_dense = tf.keras.layers.Dense(len(target_word2index)+1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Checkpoint\n",
    "import os\n",
    "checkpoint_name = os.path.join('checkpoints','Weights-{epoch:03d}--{val_loss:.5f}.hdf5')\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_weights_only = True, mode ='auto')\n",
    "\n",
    "import datetime\n",
    "log_dir = \"logs\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)\n",
    "\n",
    "callbacks_list = [checkpoint,tensorboard_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logs_base_dir = \"logs\"\n",
    "# os.makedirs(logs_base_dir, exist_ok=True)\n",
    "# %tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# start training\n",
    "model.fit(generate_batch(X_train,y_train,batch_size=128),epochs = 10,verbose =2,callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
