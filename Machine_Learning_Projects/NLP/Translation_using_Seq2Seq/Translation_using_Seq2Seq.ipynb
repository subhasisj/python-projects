{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will implement a Neural Machine Translation with LSTM using Teacher Forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import multiprocessing as mp\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "from dask.diagnostics import ProgressBar\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "from string import digits\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                    source                           target  \\\n33942              I'm sure you knew that.  Estoy seguro de que sabías eso.   \n22287                 She works at a bank.        Ella trabaja en un banco.   \n85037  What will you have for lunch today?             ¿Qué almorzarás hoy?   \n35689              Tom wouldn't lie to me.              Tom no me mentiría.   \n79042   He talks as if he knew everything.   Habla como si supiera de todo.   \n\n                                                comments  \n33942  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n22287  CC-BY 2.0 (France) Attribution: tatoeba.org #8...  \n85037  CC-BY 2.0 (France) Attribution: tatoeba.org #8...  \n35689  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n79042  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>target</th>\n      <th>comments</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>33942</th>\n      <td>I'm sure you knew that.</td>\n      <td>Estoy seguro de que sabías eso.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n    </tr>\n    <tr>\n      <th>22287</th>\n      <td>She works at a bank.</td>\n      <td>Ella trabaja en un banco.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #8...</td>\n    </tr>\n    <tr>\n      <th>85037</th>\n      <td>What will you have for lunch today?</td>\n      <td>¿Qué almorzarás hoy?</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #8...</td>\n    </tr>\n    <tr>\n      <th>35689</th>\n      <td>Tom wouldn't lie to me.</td>\n      <td>Tom no me mentiría.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n    </tr>\n    <tr>\n      <th>79042</th>\n      <td>He talks as if he knew everything.</td>\n      <td>Habla como si supiera de todo.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "data = pd.read_table('./Data/spa.txt',names=['source','target','comments'])\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<Client: 'tcp://127.0.0.1:55300' processes=4 threads=8, memory=16.00 GB>",
      "text/html": "<table style=\"border: 2px solid white;\">\n<tr>\n<td style=\"vertical-align: top; border: 0px solid white\">\n<h3 style=\"text-align: left;\">Client</h3>\n<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n  <li><b>Scheduler: </b>tcp://127.0.0.1:55300</li>\n  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n</ul>\n</td>\n<td style=\"vertical-align: top; border: 0px solid white\">\n<h3 style=\"text-align: left;\">Cluster</h3>\n<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n  <li><b>Workers: </b>4</li>\n  <li><b>Cores: </b>8</li>\n  <li><b>Memory: </b>16.00 GB</li>\n</ul>\n</td>\n</tr>\n</table>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "num_partitions = 10 #number of partitions to split dataframe\n",
    "num_cores = mp.cpu_count() #number of cores on your machine\n",
    "Client(n_workers=4, threads_per_worker=2, memory_limit='4GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML Tags\n",
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text,'lxml')\n",
    "    html_free_text = soup.get_text()\n",
    "    return html_free_text\n",
    "\n",
    "# Remove Punctuations\n",
    "special_characters= set(string.punctuation)\n",
    "def punctuation_remover(text):\n",
    "    punctuation_free_text = \" \".join([word for word in text if word \\\n",
    "                                    not in special_characters])\n",
    "    return punctuation_free_text\n",
    "\n",
    "\n",
    "# Convert to lower case\n",
    "def convert_to_lowercase(tokens):\n",
    "    low = []\n",
    "    for tok in tokens:\n",
    "        low.append(tok.lower().strip())\n",
    "    return low\n",
    "\n",
    "# Lemmatization\n",
    "def lemmatize_words(text):\n",
    "    words = nlp(str(text))\n",
    "    return [word.lemma_ for word in words if word.lemma_ != '-PRON-']  \n",
    "\n",
    "\n",
    "num_digits= str.maketrans('','', digits)\n",
    "remove_digits = lambda x: x.translate(num_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_for_tasks(text, for_pos_tagging = False):\n",
    "    cleaned_text = remove_html(text)\n",
    "    cleaned_text = word_tokenize(cleaned_text)\n",
    "    cleaned_text = lemmatize_words(cleaned_text)\n",
    "    cleaned_text = punctuation_remover(cleaned_text)\n",
    "    cleaned_text = remove_digits(cleaned_text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Time Taken for Processing 123770 rows Sequentially : 3236.405047416687 \n"
    }
   ],
   "source": [
    " start = time.time()\n",
    " data['cleaned_source'] = data.source.apply(clean_text_for_tasks)\n",
    " data['cleaned_target'] = data.target.apply(clean_text_for_tasks)\n",
    " print(f'Time Taken for Processing {data.shape[0]} rows Sequentially : {time.time()-start} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('./Data/cleaned_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  source   target                                           comments  \\\n0    Go.      Ve.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...   \n1    Go.    Vete.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...   \n2    Go.    Vaya.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...   \n3    Go.  Váyase.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...   \n4    Hi.    Hola.  CC-BY 2.0 (France) Attribution: tatoeba.org #5...   \n\n  cleaned_source cleaned_target  \n0             go             Ve  \n1             go           Vete  \n2             go           Vaya  \n3             go         Váyase  \n4             hi           Hola  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>target</th>\n      <th>comments</th>\n      <th>cleaned_source</th>\n      <th>cleaned_target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Go.</td>\n      <td>Ve.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n      <td>go</td>\n      <td>Ve</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Go.</td>\n      <td>Vete.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n      <td>go</td>\n      <td>Vete</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Go.</td>\n      <td>Vaya.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n      <td>go</td>\n      <td>Vaya</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Go.</td>\n      <td>Váyase.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n      <td>go</td>\n      <td>Váyase</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Hi.</td>\n      <td>Hola.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n      <td>hi</td>\n      <td>Hola</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if GPU's are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Num GPUs Available:  1\n"
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagginng Sentences with BOS and EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.cleaned_target = data.cleaned_target.apply(lambda x : 'START_ '+ x + ' _END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "source            0.0\ntarget            0.0\ncomments          0.0\ncleaned_source    0.0\ncleaned_target    0.0\ndtype: float64\nTime Taken for Processing 123770 rows with Dask(4 Workers ,8 Cores) : 809.9357607364655 \n"
    }
   ],
   "source": [
    "dask_df = dd.from_pandas(data,npartitions=12)\n",
    "missing_values = dask_df.isnull().sum()\n",
    "with ProgressBar():\n",
    "    missing_percent = ((missing_values / dask_df.index.size)*100).compute()\n",
    "print(missing_percent) \n",
    "\n",
    "def clean_df(df):\n",
    "    df['cleaned_source_w_dask'] = df.source.map(clean_text_for_tasks)\n",
    "    df['cleaned_target_w_dask'] = df.target.map(clean_text_for_tasks)\n",
    "    return df\n",
    "\n",
    "data['cleaned_source_w_dask'] = -1\n",
    "data['cleaned_target_w_dask'] = -1\n",
    "\n",
    "start = time.time()\n",
    "result = dask_df.map_partitions(clean_df,meta=data)\n",
    "\n",
    "df = result.compute()\n",
    "print(f'Time Taken for Processing {df.shape[0]} rows with Dask(4 Workers ,8 Cores) : {time.time()-start} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000 # max no. of words for tokenizer , Top 5000 Words in the Vocabulary\n",
    "MAX_SEQUENCE_LENGTH = 200 # max length of each entry (sentence)\n",
    "EMBEDDING_DIM = 300      # embedding dimensions for word vectors\n",
    "OOV_TOKEN = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_creator(texts,VOCAB_SIZE=VOCAB_SIZE):\n",
    "     tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE)\n",
    "     tokenizer.fit_on_texts(texts)\n",
    "     word_index_dictionary = tokenizer.word_index\n",
    "\n",
    "     word2index = {}\n",
    "     index2word = {}\n",
    "     for key,value in word_index_dictionary.items():\n",
    "         if value < VOCAB_SIZE:\n",
    "             word2index[key] = value\n",
    "             index2word[value] = key\n",
    "         if value >= VOCAB_SIZE-1:\n",
    "             continue\n",
    "     return word2index ,index2word\n",
    "\n",
    "\n",
    "\n",
    "# return word_index\n",
    "# dict(list(word_index.items())[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_word2index ,source_index2word = vocab_creator(data.cleaned_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{1: 'be',\n 2: 'the',\n 3: 'to',\n 4: 'tom',\n 5: 'do',\n 6: 'a',\n 7: \"n't\",\n 8: 'have',\n 9: \"'s\",\n 10: 'that',\n 11: 'in',\n 12: 'of',\n 13: 'this',\n 14: 'go',\n 15: 'for'}"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "dict(list(source_index2word.items())[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word2index ,target_index2word = vocab_creator(data.cleaned_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'start': 1,\n 'end': 2,\n 'de': 3,\n 'que': 4,\n 'no': 5,\n 'a': 6,\n 'tom': 7,\n 'la': 8,\n '¿': 9,\n 'el': 10,\n 'en': 11,\n 'es': 12,\n 'un': 13,\n 'se': 14,\n 'por': 15}"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "dict(list(target_word2index.items())[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((111393,), (12377,))"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.cleaned_source, df.cleaned_target, test_size = 0.1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}