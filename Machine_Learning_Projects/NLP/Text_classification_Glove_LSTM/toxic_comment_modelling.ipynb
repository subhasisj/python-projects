{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Toxic Comments using GloVe and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                 id                                       comment_text  toxic  \\\n0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n\n   severe_toxic  obscene  threat  insult  identity_hate  \\\n0             0        0       0       0              0   \n1             0        0       0       0              0   \n2             0        0       0       0              0   \n3             0        0       0       0              0   \n4             0        0       0       0              0   \n\n                                        cleaned_text  \n0  explanation why edit make username Hardcore Me...  \n1  D'aww match background colour m seemingly stuc...  \n2  hey man m really try edit war 's guy constantl...  \n3  More ca n't make real suggestion improvement w...  \n4               sir hero any chance remember page 's  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n      <th>cleaned_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>explanation why edit make username Hardcore Me...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>D'aww match background colour m seemingly stuc...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>hey man m really try edit war 's guy constantl...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>More ca n't make real suggestion improvement w...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>sir hero any chance remember page 's</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Load the cleaned dataset\n",
    "dataset = pd.read_csv('./data/toxic_comments_cleaned.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000 # max no. of words for tokenizer , Top 5000 Words in the Vocabulary\n",
    "MAX_SEQUENCE_LENGTH = 200 # max length of each entry (sentence)\n",
    "EMBEDDING_DIM = 300      # embedding dimensions for word vectors\n",
    "GLOVE_DIR = f\"./GloVe/glove.42B.{EMBEDDING_DIM}d.txt\"\n",
    "OOV_TOKEN = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(dataset.columns[2:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset[labels].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE,\n",
    "                                                   oov_token=OOV_TOKEN)\n",
    "tokenizer.fit_on_texts(X)\n",
    "word_index = tokenizer.word_index\n",
    "dict(list(word_index.items())[:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Vocabulary size:31785\n"
    }
   ],
   "source": [
    "print(f'Vocabulary size:{len(word_index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Text Sequences for Sentence 10\nLength of Sequence :286\n----------------------------------------------\n[212, 10, 846, 42, 1, 385, 104, 652, 42, 1, 385, 177, 42, 5, 2203, 42, 10, 212, 10, 506, 846, 10, 8, 3, 1764, 212, 10, 63, 462, 1, 212, 10, 255, 210, 28, 80, 42, 670, 5, 367, 506, 846, 10, 42, 3, 2204, 212, 10, 17, 16, 42, 670, 5, 15, 102, 212, 10, 846, 20, 699, 212, 10, 351, 124, 174, 1021, 212, 10, 846, 5, 52, 76, 42, 5, 15, 723, 2, 234, 2, 2, 47, 1275, 282, 8, 5, 612, 1190, 2, 42, 2, 2, 2704, 712, 112, 212, 10, 42, 699, 322, 27, 507, 546, 506, 58, 13, 412, 699, 406, 280, 240, 64, 20, 79, 17, 90, 351, 182, 79, 5, 44, 9, 339, 531, 339, 1, 25, 42, 1, 385, 104, 652, 42, 1, 385, 177, 303, 4, 670, 5, 437, 6, 1021, 109, 118, 182, 659, 1800, 20, 109, 303, 39, 1021, 1576, 182, 20, 1963, 219, 47, 219, 40, 724, 1, 219, 4, 204, 10, 118, 913, 1481, 57, 161, 182, 1684, 213, 219, 4, 1801, 182, 28, 1258, 128, 32, 36, 25, 17, 36, 642, 182, 2602, 134, 303, 6, 13, 214, 20, 109, 40, 330, 2968, 633, 134, 10, 475, 2141, 20, 143, 351, 528, 280, 8, 212, 10, 10, 134, 13, 134, 76, 8, 42, 182, 134, 212, 10, 19, 8, 42, 182, 134, 444, 76, 182, 134, 10, 20, 699, 303, 124, 174, 1021, 25, 134, 52, 76, 303, 699, 167, 47, 1094, 4591, 42, 27, 58, 13, 412, 134, 406, 280, 240, 64, 20, 42, 182, 230, 149, 671, 407, 8, 212, 10, 42, 58, 1516, 468, 20, 79, 17, 90, 351, 182, 79, 5, 44, 9, 339, 531, 339]\n"
    }
   ],
   "source": [
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "print('Text Sequences for Sentence 10')\n",
    "print(f'Length of Sequence :{len(X_sequences[10])}')\n",
    "print('----------------------------------------------')\n",
    "print(X_sequences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Length of Padded Sequence :200\n"
    }
   ],
   "source": [
    "# Padding of Sequence to make all sentences of uniform size\n",
    "X_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(X_sequences,\n",
    "maxlen=MAX_SEQUENCE_LENGTH,\n",
    "truncating='post',\n",
    "padding='post')\n",
    "\n",
    "print(f'Length of Padded Sequence :{len(X_sequences_padded[10])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Shape of dataset Sequence :(10000, 200)\nShape of Label :(10000, 6)\n"
    }
   ],
   "source": [
    "print(f'Shape of dataset Sequence :{X_sequences_padded.shape}')\n",
    "print(f'Shape of Label :{y.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split into Train and Test\n",
    "\n",
    "# X_train,X_valid,y_train,y_valid = train_test_split(news_data['cleaned_text'],news_data['category'],test_size = 0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the GloVe Vector in a Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_index(glove_dir): \n",
    "    \n",
    "    embeddings_index = {}\n",
    "    f = open(glove_dir,encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        embeddings_index[word] = np.asarray(values[1:],dtype='float32')\n",
    "    f.close()\n",
    "\n",
    "    return embeddings_index       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = create_embeddings_index(GLOVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embeddings_matrix = np.zeros((len(word_index)+1,EMBEDDING_DIM))\n",
    "for word,i in word_index.items():\n",
    "    embeddings_vector = embeddings_index.get(word)\n",
    "    if embeddings_vector is not None:\n",
    "        embeddings_matrix[i] = embeddings_vector\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {}
    }
   ],
   "source": [
    "logs_base_dir = \"logs\"\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "%tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build the Model\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     # Embedding Layer \n",
    "#     tf.keras.layers.Embedding(input_dim=len(word_index)+1,\n",
    "#                               output_dim=EMBEDDING_DIM,\n",
    "#                               weights = [embeddings_matrix],\n",
    "#                               input_length=MAX_SEQUENCE_LENGTH,\n",
    "#                               trainable=False,\n",
    "#                               name = 'embeddings'\n",
    "#                               ),\n",
    "#     # Bidiretional LSTM for learning Long term dependencies\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128,return_sequences=True)),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64,return_sequences=True)),\n",
    "#     tf.keras.layers.Dropout(0.5),\n",
    "#     # Dense Layer with RELU\n",
    "#     tf.keras.layers.Dense(50,activation='relu'),\n",
    "#     tf.keras.layers.Dropout(0.5),\n",
    "#     # Ouput layer with 6 units beacuse label Tokenizer starts with 1 but \n",
    "#     # sparse_categorical_crossentropy loss function thinks 0 as a possible \n",
    "#     # label as well so we have to give labels [0,1,2,3,4,5] even though \n",
    "#     # 0 is not used\n",
    "#     tf.keras.layers.Dense(6,activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')\n",
    "embedding_layer = tf.keras.layers.Embedding(len(word_index)+1,\n",
    "                                            output_dim=EMBEDDING_DIM,        \n",
    "                                            weights = [embeddings_matrix],\n",
    "                                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                            trainable=False,\n",
    "                                            name = 'embeddings' )\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x =  tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128,return_sequences=True))(embedded_sequences)      \n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(50,activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "preds = tf.keras.layers.Dense(6,activation='sigmoid')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model(sequence_input,preds)\n",
    "model.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "log_dir = \"logs\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 6699 samples, validate on 3301 samples\nEpoch 1/10\n6699/6699 [==============================] - 248s 37ms/sample - loss: 0.2029 - accuracy: 0.9565 - val_loss: 0.1094 - val_accuracy: 0.9661\nEpoch 2/10\n6699/6699 [==============================] - 233s 35ms/sample - loss: 0.0840 - accuracy: 0.9685 - val_loss: 0.0642 - val_accuracy: 0.9778\nEpoch 3/10\n6699/6699 [==============================] - 220s 33ms/sample - loss: 0.0620 - accuracy: 0.9796 - val_loss: 0.0579 - val_accuracy: 0.9805\nEpoch 4/10\n6699/6699 [==============================] - 238s 36ms/sample - loss: 0.0578 - accuracy: 0.9802 - val_loss: 0.0584 - val_accuracy: 0.9809\nEpoch 5/10\n6699/6699 [==============================] - 205s 31ms/sample - loss: 0.0549 - accuracy: 0.9808 - val_loss: 0.0596 - val_accuracy: 0.9795\nEpoch 6/10\n6699/6699 [==============================] - 238s 35ms/sample - loss: 0.0510 - accuracy: 0.9823 - val_loss: 0.0569 - val_accuracy: 0.9812\nEpoch 7/10\n6699/6699 [==============================] - 245s 37ms/sample - loss: 0.0481 - accuracy: 0.9829 - val_loss: 0.0602 - val_accuracy: 0.9806\nEpoch 8/10\n6699/6699 [==============================] - 222s 33ms/sample - loss: 0.0479 - accuracy: 0.9830 - val_loss: 0.0572 - val_accuracy: 0.9811\nEpoch 9/10\n6699/6699 [==============================] - 209s 31ms/sample - loss: 0.0437 - accuracy: 0.9845 - val_loss: 0.0658 - val_accuracy: 0.9800\nEpoch 10/10\n6699/6699 [==============================] - 205s 31ms/sample - loss: 0.0415 - accuracy: 0.9839 - val_loss: 0.0657 - val_accuracy: 0.9802\n"
    }
   ],
   "source": [
    "try:\n",
    "    history = model.fit(X_sequences_padded, y,\n",
    "                        epochs=10, batch_size=64,\n",
    "                        validation_split=0.33,\n",
    "                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=10),\n",
    "                                   tensorboard_callback],\n",
    "                        verbose=1)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    model.save('multilabel_classification_model.h5')\n",
    "    print('Model Saved because of user input')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./multi_label_toxic_comment_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}