{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Text Classification using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = pd.read_csv('https://raw.githubusercontent.com/susanli2016/PyCon-Canada-2019-NLP-Tutorial/master/bbc-text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            511\n",
       "business         510\n",
       "politics         417\n",
       "tech             401\n",
       "entertainment    386\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check count of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "# import plotly\n",
    "# plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "# df = px.data.tips()\n",
    "# # fig = px.histogram(df, x=\"sex\", y=\"tip\", histfunc=\"avg\", color=\"smoker\", barmode=\"group\",\n",
    "# #              facet_row=\"time\", facet_col=\"day\", category_orders={\"day\": [\"Thur\", \"Fri\", \"Sat\", \"Sun\"],\n",
    "# #                                                                 \"time\": [\"Lunch\", \"Dinner\"]})\n",
    "# fig = px.bar(news_data )\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "\n",
    "# Remove HTML Tags\n",
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text,'lxml')\n",
    "    html_free_text = soup.get_text()\n",
    "    return html_free_text\n",
    "\n",
    "# Remove Punctuations\n",
    "def punctuation_remover(text):\n",
    "    punctuation_free_text = \" \".join([word for word in text if word \\\n",
    "                                    not in string.punctuation])\n",
    "    return punctuation_free_text\n",
    "\n",
    "\n",
    "# Stop Word Removal\n",
    "# cached_stop_words = stopwords.words('english') # Provides 70 X Speedup\n",
    "def stop_words_remover(text):\n",
    "#     text = text.lower().split()\n",
    "    words = [word for word in text if \\\n",
    "             word not in stopwords]\n",
    "    return words\n",
    "\n",
    "# Convert to lower case\n",
    "def convert_to_lowercase(tokens):\n",
    "    low = []\n",
    "    for tok in tokens:\n",
    "        low.append(tok.lower().strip())\n",
    "    return low\n",
    "\n",
    "# # Lemmatization\n",
    "# def lemmatize_words(text):\n",
    "#     words = nlp(str(text))\n",
    "#     return [word.lemma_ for word in words if word.lemma_ != '-PRON-']  \n",
    "\n",
    "# def replace_urls(tokens):\n",
    "#     re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", tokens)\n",
    "#     return tokens\n",
    "\n",
    "\n",
    "# def remove_short_strings(text, length):\n",
    "#     array = []\n",
    "#     for word in text:\n",
    "#         if len(word) > length:\n",
    "#             array.append(word)\n",
    "#     return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_for_tasks(text, for_pos_tagging = False):\n",
    "    cleaned_text = remove_html(text)\n",
    "#     cleaned_text = replace_urls(cleaned_text)\n",
    "    cleaned_text = word_tokenize(cleaned_text)\n",
    "    cleaned_text = stop_words_remover(cleaned_text)\n",
    "    \n",
    "#     if for_pos_tagging is False:\n",
    "#         cleaned_text = convert_to_lowercase(cleaned_text)\n",
    "#         cleaned_text = stop_words_remover(cleaned_text)\n",
    "#         cleaned_text = lemmatize_words(cleaned_text)\n",
    "#     cleaned_text = remove_short_strings(cleaned_text,2)\n",
    "    cleaned_text = punctuation_remover(cleaned_text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\i323570\\Anaconda3\\envs\\NLP\\lib\\site-packages\\tqdm\\std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b393731f2e440009cb3d0c33c8177c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2225.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "tqdm_notebook.pandas()\n",
    "# from pandarallel import pandarallel\n",
    "# pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "news_data['cleaned_text'] = news_data.text.progress_apply(clean_text_for_tasks)\n",
    "# tokenized_documents['cleaned_text'] = tokenized_documents.text.parallel_apply(clean_text_for_tasks,axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future hands viewers home theatre systems p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss left books alone former worldcom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary farrell gamble leicester say rushe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle fa cup premiership side...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean twelve raids box office ocean twelve cri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text  \\\n",
       "0           tech  tv future in the hands of viewers with home th...   \n",
       "1       business  worldcom boss  left books alone  former worldc...   \n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3          sport  yeading face newcastle in fa cup premiership s...   \n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  tv future hands viewers home theatre systems p...  \n",
       "1  worldcom boss left books alone former worldcom...  \n",
       "2  tigers wary farrell gamble leicester say rushe...  \n",
       "3  yeading face newcastle fa cup premiership side...  \n",
       "4  ocean twelve raids box office ocean twelve cri...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_valid,y_train,y_valid = train_test_split(news_data['cleaned_text'],news_data['category'],test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000 # Choose 5000 top words in the Vocabulary\n",
    "embedding_dim = 64\n",
    "max_length = 200\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = vocab_size,\n",
    "                                                  oov_token=oov_tok\n",
    "                                                 )\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'said': 2,\n",
       " 'mr': 3,\n",
       " 'would': 4,\n",
       " 'year': 5,\n",
       " 'also': 6,\n",
       " 'us': 7,\n",
       " 'new': 8,\n",
       " 'people': 9,\n",
       " 'one': 10,\n",
       " 'could': 11,\n",
       " 'last': 12,\n",
       " 'first': 13,\n",
       " 'time': 14,\n",
       " 'two': 15}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(word_index.items())[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1390, 903, 611, 1, 1, 49, 2020, 24, 113, 132, 1610, 2598, 1, 2776, 1013, 2020, 1, 16, 36, 1167, 545, 4732, 903, 611, 5, 466, 174, 94, 1, 703, 1, 647, 1, 209, 1, 4059, 1, 1, 403, 3859, 2074, 1, 1, 128, 1, 1, 1036, 5, 110, 1, 2, 334, 1, 3860, 105, 4245, 2777, 1, 1, 762, 241, 520, 39, 1432, 168, 264, 2, 24, 140, 667, 4, 616, 1432, 648, 174, 6, 762, 241, 718]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_sequences[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will add padding to the input sequences so as to make all inputs of the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(X_train_sequences,\n",
    "                                                                      maxlen=max_length,\n",
    "                                                                      truncating=trunc_type,\n",
    "                                                                      padding=padding_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input before padding 492\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of input before padding {len(X_train_sequences[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input after padding 200\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of input after padding {len(X_train_padded_inputs[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_sequences = tokenizer.texts_to_sequences(X_valid)\n",
    "X_valid_padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(X_valid_sequences,\n",
    "                                                                      maxlen = max_length,\n",
    "                                                                      truncating=trunc_type,\n",
    "                                                                      padding=padding_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(668, 200)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_padded_inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now all input sequences are of the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's tokenize the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tokenizer = tf.keras.preprocessing.text.Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tokenizer.fit_on_texts(news_data.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sequences = np.array(label_tokenizer.texts_to_sequences(y_train))\n",
    "y_valid_sequences = np.array(label_tokenizer.texts_to_sequences(y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_valid_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So the 5 textual categories have been converted into numeric ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<OOV> suspended drugs test <OOV> <OOV> says banned three months international tennis federation <OOV> testing positive banned <OOV> world number 60 failed routine drugs test year french open plans <OOV> appeal <OOV> believes <OOV> given <OOV> doctor <OOV> <OOV> injury blame producing <OOV> <OOV> system <OOV> <OOV> 27 year old <OOV> said statement <OOV> defeated britain greg rusedski <OOV> <OOV> davis cup september set miss start season said three month ban would mean miss australian open also davis cup australia ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value,key) for (key,value) in word_index.items()])\n",
    "def decode_article(text):\n",
    "    return ' '.join([reverse_word_index.get(i,'?') for i in text])\n",
    "\n",
    "print(decode_article(X_train_padded_inputs[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Many words have been replaced by OOV as they do not constitute the top 5000 words of the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 25328), started 0:03:02 ago. (Use '!kill 25328' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-388fc9572cafd1a4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-388fc9572cafd1a4\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "logs_base_dir = \"logs\"\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "%tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          320000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 394,694\n",
      "Trainable params: 394,694\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # Embedding Layer \n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size,output_dim=embedding_dim),\n",
    "    # Bidiretional LSTM for learning Long term dependencies\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "    # Dense Layer with RELU\n",
    "    tf.keras.layers.Dense(embedding_dim,activation='relu'),\n",
    "    # Ouput layer with 6 units beacuse label Tokenizer starts with 1 but \n",
    "    # sparse_categorical_crossentropy loss function thinks 0 as a possible \n",
    "    # label as well so we have to give labels [0,1,2,3,4,5] even though \n",
    "    # 0 is not used\n",
    "    tf.keras.layers.Dense(6,activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "log_dir = \"logs\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1557 samples, validate on 668 samples\n",
      "Epoch 1/10\n",
      "1557/1557 [==============================] - 17s 11ms/sample - loss: 1.6816 - accuracy: 0.2884 - val_loss: 1.5136 - val_accuracy: 0.3308\n",
      "Epoch 2/10\n",
      "1557/1557 [==============================] - 10s 6ms/sample - loss: 1.4531 - accuracy: 0.3385 - val_loss: 1.3144 - val_accuracy: 0.4476\n",
      "Epoch 3/10\n",
      "1557/1557 [==============================] - 8s 5ms/sample - loss: 1.0050 - accuracy: 0.6397 - val_loss: 0.7986 - val_accuracy: 0.7275\n",
      "Epoch 4/10\n",
      "1557/1557 [==============================] - 8s 5ms/sample - loss: 0.5611 - accuracy: 0.8234 - val_loss: 0.5515 - val_accuracy: 0.8084\n",
      "Epoch 5/10\n",
      "1557/1557 [==============================] - 7s 5ms/sample - loss: 0.2485 - accuracy: 0.9550 - val_loss: 0.2598 - val_accuracy: 0.9162\n",
      "Epoch 6/10\n",
      "1557/1557 [==============================] - 9s 6ms/sample - loss: 0.1124 - accuracy: 0.9672 - val_loss: 0.1888 - val_accuracy: 0.9491\n",
      "Epoch 7/10\n",
      "1557/1557 [==============================] - 8s 5ms/sample - loss: 0.0330 - accuracy: 0.9936 - val_loss: 0.2170 - val_accuracy: 0.9491\n",
      "Epoch 8/10\n",
      "1557/1557 [==============================] - 8s 5ms/sample - loss: 0.0228 - accuracy: 0.9968 - val_loss: 0.2037 - val_accuracy: 0.9386\n",
      "Epoch 9/10\n",
      "1557/1557 [==============================] - 9s 5ms/sample - loss: 0.0121 - accuracy: 0.9994 - val_loss: 0.1721 - val_accuracy: 0.9521\n",
      "Epoch 10/10\n",
      "1557/1557 [==============================] - 10s 6ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.1737 - val_accuracy: 0.9566\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    history = model.fit(X_train_padded_inputs, y_train_sequences,\n",
    "                        epochs=10, batch_size=64,\n",
    "                        validation_data = (X_valid_padded_inputs,y_valid_sequences),\n",
    "                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=10),\n",
    "                                   tensorboard_callback],\n",
    "                        verbose=1)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    model.save('multiclass_classification_model.h5')\n",
    "    print('Model Saved because of user input')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"\"\"Fifa tournament 'a great idea'\n",
    "Along with keeping fit so he is ready if football resumes, Jota has been busy competing in the ePremier League Invitational Fifa 20 tournament.\n",
    "\n",
    "He beat Alexander-Arnold with a golden goal in Saturday's final, a match the Portugal international described as “very tense”.\n",
    "\n",
    "Jota says his Fifa success is partly down to using set-pieces he would play in real life on the video game and hailed the tournament as “a great idea”.\n",
    "\n",
    "“Trent Alexander-Arnold was a very good competitor,” he added.\n",
    "\n",
    "“I needed to be very focussed because one mistake would cost me the tournament.\n",
    "\n",
    "“We were playing for a good cause, to help the NHS, and give the fans something to watch. It’s sad for everyone this period we are living and we need to keep the fans busy.\n",
    "\n",
    "\"This was a great idea. To be crowned champion makes everything even better.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_cleaned = clean_text_for_tasks(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Fifa tournament great idea' Along keeping fit ready football resumes Jota busy competing ePremier League Invitational Fifa 20 tournament He beat Alexander-Arnold golden goal Saturday 's final match Portugal international described “ tense ” Jota says Fifa success partly using set-pieces would play real life video game hailed tournament “ great idea ” “ Trent Alexander-Arnold good competitor ” added “ I needed focussed one mistake would cost tournament “ We playing good cause help NHS give fans something watch It ’ sad everyone period living need keep fans busy `` This great idea To crowned champion makes everything even better\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_sequence = tokenizer.texts_to_sequences(test_sentence_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3112, 1, 3112, 2464, 1488, 1, 1, 1119, 2011, 2464, 1, 239, 2011, 1488, 1008, 1119, 239, 2464, 1488, 1, 1, 239, 2464, 1, 2464, 1873, 1, 2011, 1008, 3332, 239, 239, 1950, 1, 2011, 1008, 3112, 1, 1488, 1119, 239, 2464, 1, 1, 3112, 1, 1, 1488, 1140, 2464, 1873, 1873, 1119, 239, 4560, 1, 1, 239, 4560, 975, 1, 1488, 2464, 1140, 1, 4560, 1, 1863, 1, 1, 1950, 239, 1488, 1, 2011, 1008, 239, 1950, 1119, 239, 1, 1, 239, 1119, 1873, 239, 2464, 1008, 1, 239, 1, 2011, 943, 1, 1488, 2464, 1488, 1, 1, 2011, 2464, 1873, 3112, 1, 3112, 2464, 44, 287, 1488, 1, 1, 1119, 2011, 2464, 1, 239, 2011, 1488, 3689, 239, 1140, 239, 2464, 1488, 2464, 1873, 239, 2708, 2464, 2011, 1, 239, 1119, 2464, 1119, 2011, 1, 1873, 1, 1008, 1, 1873, 1, 239, 2011, 1008, 1, 2464, 1873, 4560, 2464, 1488, 1, 1119, 1, 2464, 1, 1, 4560, 3112, 1, 2011, 2464, 1873, 1, 2464, 1488, 1863, 3689, 1950, 1, 1119, 1488, 1, 1008, 2464, 1873, 1, 2011, 1488, 239, 1119, 2011, 2464, 1488, 1, 1, 2011, 2464, 1873, 1, 239, 4560, 1863, 1119, 1, 1140, 239, 1, 1, 1488, 239, 2011, 4560, 239, 1, 975, 1, 1488, 2464, 4560, 2464, 1, 4560, 3112, 1, 3112, 2464, 4560, 1, 1863, 1863, 239, 4560, 4560, 1950, 2464, 1119, 1488, 1873, 1, 1, 4560, 1, 2011, 1008, 4560, 239, 1488, 1950, 1, 239, 1863, 239, 4560, 2526, 1, 1, 1873, 1, 1950, 1873, 2464, 1, 1119, 239, 2464, 1873, 1873, 1, 3112, 239, 943, 1, 1, 239, 1, 1008, 2464, 1, 239, 3689, 2464, 1, 1873, 239, 1, 1488, 1, 1, 1119, 2011, 2464, 1, 239, 2011, 1488, 1, 1008, 1119, 239, 2464, 1488, 1, 1, 239, 2464, 1, 1, 1488, 1119, 239, 2011, 1488, 2464, 1873, 239, 2708, 2464, 2011, 1, 239, 1119, 2464, 1119, 2011, 1, 1873, 1, 1008, 1, 1, 1, 1863, 1, 1, 1950, 239, 1488, 1, 1488, 1, 1119, 1, 2464, 1, 1, 239, 1, 1, 1, 2011, 239, 239, 1, 239, 1, 3112, 1, 1863, 1, 4560, 4560, 239, 1, 1, 2011, 239, 1, 1, 4560, 1488, 2464, 3332, 239, 2526, 1, 1, 1873, 1, 1863, 1, 4560, 1488, 1488, 1, 1, 1119, 2011, 2464, 1, 239, 2011, 1488, 1, 2526, 239, 1950, 1873, 2464, 1, 1, 2011, 1008, 1008, 1, 1, 1, 1863, 2464, 1, 4560, 239, 3689, 239, 1873, 1950, 2011, 3689, 4560, 1008, 1, 943, 239, 3112, 2464, 2011, 4560, 4560, 1, 1, 239, 1488, 3689, 1, 2011, 1008, 2526, 2464, 1488, 1863, 3689, 1, 1488, 1, 4560, 2464, 1, 239, 943, 239, 1119, 1, 1, 2011, 239, 1950, 239, 1119, 1, 1, 1, 1873, 1, 943, 1, 2011, 1008, 2011, 239, 239, 1, 3332, 239, 239, 1950, 3112, 2464, 2011, 4560, 1140, 1, 4560, 1, 1488, 3689, 1, 4560, 1008, 1119, 239, 2464, 1488, 1, 1, 239, 2464, 1488, 1, 1863, 1119, 1, 2526, 2011, 239, 1, 1863, 3689, 2464, 1, 1950, 1, 1, 2011, 1, 2464, 3332, 239, 4560, 239, 943, 239, 1119, 1, 1488, 3689, 1, 2011, 1008, 239, 943, 239, 2011, 1140, 239, 1488, 1488, 239, 1119]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def flatten_text_sequence(text):\n",
    "    flatten = itertools.chain.from_iterable\n",
    "    text = list(flatten(text))\n",
    "    return text\n",
    "\n",
    "test_sentence_sequence = flatten_text_sequence(test_sentence_sequence)\n",
    "\n",
    "print(test_sentence_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_padded = tf.keras.preprocessing.sequence.pad_sequences([test_sentence_sequence],\n",
    "                                                                      maxlen=max_length,\n",
    "                                                                      truncating=trunc_type,\n",
    "                                                                      padding=padding_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3112,    1, 3112, 2464, 1488,    1,    1, 1119, 2011, 2464,    1,\n",
       "         239, 2011, 1488, 1008, 1119,  239, 2464, 1488,    1,    1,  239,\n",
       "        2464,    1, 2464, 1873,    1, 2011, 1008, 3332,  239,  239, 1950,\n",
       "           1, 2011, 1008, 3112,    1, 1488, 1119,  239, 2464,    1,    1,\n",
       "        3112,    1,    1, 1488, 1140, 2464, 1873, 1873, 1119,  239, 4560,\n",
       "           1,    1,  239, 4560,  975,    1, 1488, 2464, 1140,    1, 4560,\n",
       "           1, 1863,    1,    1, 1950,  239, 1488,    1, 2011, 1008,  239,\n",
       "        1950, 1119,  239,    1,    1,  239, 1119, 1873,  239, 2464, 1008,\n",
       "           1,  239,    1, 2011,  943,    1, 1488, 2464, 1488,    1,    1,\n",
       "        2011, 2464, 1873, 3112,    1, 3112, 2464,   44,  287, 1488,    1,\n",
       "           1, 1119, 2011, 2464,    1,  239, 2011, 1488, 3689,  239, 1140,\n",
       "         239, 2464, 1488, 2464, 1873,  239, 2708, 2464, 2011,    1,  239,\n",
       "        1119, 2464, 1119, 2011,    1, 1873,    1, 1008,    1, 1873,    1,\n",
       "         239, 2011, 1008,    1, 2464, 1873, 4560, 2464, 1488,    1, 1119,\n",
       "           1, 2464,    1,    1, 4560, 3112,    1, 2011, 2464, 1873,    1,\n",
       "        2464, 1488, 1863, 3689, 1950,    1, 1119, 1488,    1, 1008, 2464,\n",
       "        1873,    1, 2011, 1488,  239, 1119, 2011, 2464, 1488,    1,    1,\n",
       "        2011, 2464, 1873,    1,  239, 4560, 1863, 1119,    1, 1140,  239,\n",
       "           1,    1]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5], dtype=int64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(test_sentence_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence2 = \"\"\"For decades, the West, led by US strategic thinking, bet that full-on engagement with Beijing would alter the opaque nature of Chinese politics, making it more liberal and open. The onset of the Covid-19 pandemic should ensure a quick burial to this belief. The free and open liberal world order has run into the great political wall of China with deleterious consequences. Not only did the intense engagement with China fail to alter its politics, but many liberal democracies have also adopted Chinese-style industrial planning policies. The irony of today’s geopolitical moment is that Western taxpayers underwrote China’s bid for global influence. Successive US administrations, egged on by Big Business and Big Finance, played a crucial role in bringing China into the global community, culminating in Bill Clinton’s decision to welcome China into the WorldTrade Organisation (WTO) system.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_cleaned = clean_text_for_tasks(test_sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_sequence = tokenizer.texts_to_sequences(test_sentence_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_sequence = flatten_text_sequence(test_sentence_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_padded = tf.keras.preprocessing.sequence.pad_sequences([test_sentence_sequence],\n",
    "                                                                      maxlen=max_length,\n",
    "                                                                      truncating=trunc_type,\n",
    "                                                                      padding=padding_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3], dtype=int64)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(test_sentence_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Model Correctly predicted the label for a news related to Politics but it labelled a News Related to FIFA as entertainment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
